---
title:       "Information Entropy"
subtitle:    "信息的度量和作用"
description: "信息论之父所提出信息熵等相关概念简介"
date:        2022-06-27
author:      蔡博宇
image:       "https://personal-drawing-bed.oss-cn-beijing.aliyuncs.com/img/pexels-tobi-620337.jpg"
published: true
tags:
    - Math
URL: "/2022/6/27/Math/Pleasing/Entropy"
categories:  ["Tech" ]
---

# Information Entropy

## QA

或许一直都有一个疑问，升入大学信息是一个非常宝贵的财富，我们都知道某些关键的信息或许真能改变一生，但是我们如何分辨哪些是关键信息，或者如何确定一条信息的价值呢？可能你能凭借直观的感受来评判，但这在数学上可不好，数学更喜欢公式，喜欢确定性和可计算的东西。于是，当我读到这一章，无不感受到信息熵的魅力。

## 信息的度量

考虑这种情况，如果我们越了解一个事务，那么他的不确定性就越小，我们就不需要获取更多的信息来填补这些不确定性的空白。因此，信息量就是不确定性的多少。

![Free Stock Photo](https://personal-drawing-bed.oss-cn-beijing.aliyuncs.com/img/pexels-photo-210607.jpeg)

总感觉，这个定义是根据股票交易所来的，通常来说，你了解的信息越多，你可能赚的也越多。但其实吧，我觉得每一个人不可能会把所有的不确定性消除，生活中总有那么些不如你所愿的事情，这可能也是支持每一个冒险者清晨起床的原因吧。

### 信息熵

我们接着来看，大致方向把握住了，我们就来看看信息论之父 **香农** 的定义：

​	$$H(X) = -\sum_{x\in X}P(x)\log{P(x)}  $$

该如何理解这个公式呢，《数学之美》中的例子在这被我借鉴了。假如我们去看今年2022的世界杯，假如有一个人穿越而来，他知道冠军是谁，但是她不愿直接告诉我而是让我去猜，每猜一次都要花费一元，该如何最快得到答案呢？

![Binaryerasurechannel1.png](https://personal-drawing-bed.oss-cn-beijing.aliyuncs.com/img/Binaryerasurechannel.png)

我相信各位都知道二分查找的算法，总共32支球队，我先询问在1-16号中吗？依次询问下去，我们最多只需要五次就可以得到答案 ($log{32} = 5$)。

带入上式公式，可知每个队伍的概率都是 $\frac{1}{32} $,最后化简即为 $log{32} = 5$。

然而我们知道，每个队伍实力不同，我们可以优先在夺冠热门的队伍中选择，这样我们付的钱就最少，代价就更低。当然可以用数学证明:

$$H(X) \le -log{\frac{1}{n} } $$ (n为球队总个数)

利用信息熵，我们大概也知道信息存储时的特点，在一本书中，10%的文本占常用字的95%以上，因此可以计算一本书的信息熵是很小的，因此将它存储在电脑中时，我们进行了压缩，这样它的实际内容没有变化，但所占字节数更小了。

因此可以看出重复内容越多的书籍，信息熵越少，我们能掌握它的代价也就越小，但可能它的质量就越坏。



## 信息的作用

从上文信息的定义入手，为了度量一条信息，我们引入了信息熵来计算这条信息的大小，然而我们确定信息价值的本质还是一样的，看信息能帮我消除多少不确定性。

在此，引入了条件熵的概念，《概率论与数理统计》中我们都有学到条件概率的概念，在 $P(y)$ 发生的概率下，$P(x)$ 发生的概率表示为：$P(x|y)$ ，条件熵也是如此：

$$H(X|Y) = -\sum_{x\in X,y\in Y} P(x,y)log{P(x|y)}$$

可以证明，$H(x)\ge H(X|Y) $,由此可以看出在得到 $Y$ 的条件后，信息熵变小，$X$ 的不确定性下降了。但是也得考虑一种情况，在得到某一信息时，不确定性并没有下降，这时等号就成立了。



## 互信息

最后就是如何评价两个信息的相关性了，好比如，今天是雾天，今天可能下雨，这两条信息的相关性大吗？

因此香农提出了“互信息”的概念(`Mutual Information`)，来度量两个信息之间的相关性，定义如下：

$$I(X;Y) = \sum_{x\in X,y\in Y}P(x,y)log\frac{P(x,y)}{P(x)P(y)}  $$

其实这个公式就等于：

$$I(X;Y) = H(X) - H(X|Y)$$

即等于消除了多少的不确定性，或者条件熵之间的差异。

互信息经常用于消除词语的二义性的问题，很经典的问题是：用红墨水写一个“蓝”字，请问，这个字是红字还是蓝字？在这就需要用更多的信息去消除不确定，例如是红色的字还是单一个“红字”？

在例如，我们常使用的多义词，多音词，机器该怎么识别呢？可以通过识别与这个词相邻的词，通过互信息的大小来判断，谁的互信息大，谁的概率就高。然而这个始终是一个概率问题，他不能保证不会出错，因为贪心并不一定是全局最优解。



## 思考

其实，发明信息度量之后，很多现象都可解释和度量，但是汉语言的很多问题还是不能解释，并且很多问题仍然是概率问题，而不是一个人思维的真正过程，机器总是做着最大概率的事情，这在人类世界是不可能的，我相信真正的机器智能与自然语言处理还有不少路可以走。